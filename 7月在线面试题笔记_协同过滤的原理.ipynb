{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": "## 1.推荐引擎分类\n### 1.1 协同过滤\n- 基于用户的推荐（通过共同口味和偏好找共同邻居，K-邻居算法、你朋友喜欢，你也喜欢）\n- 基于项目的推荐（发现物品间的相似度、推荐类似物品，你喜欢A，C与A相似，也可能喜欢C）\n- 基于模型的推荐（基于样本用户喜好信息构造一个推荐模型，然后根据实时的用户喜好信息预测推荐）\n\n\u003e基于相似用户的协同过滤推荐（用户与系统或互联网交互留下的一切信息、蛛丝马迹，或用户与用户之间千丝万缕的联系），以及基于相似项目的协同过滤推荐（尽最大可能发现物品间的相似度）\n### 1.2 基于内容的分析推荐（调查问卷，电子邮件，或者推荐引擎对本blog内容的分析）\n## 2.协同过滤\n## 2.1协同过滤推荐步骤\n\n1）若要做协同过滤，那么收集用户偏好则成了关键\n\n2) 对数据进行减噪与归一化操作(得到一个用户偏好的二维矩阵，一维是用户列表，另一维是物品列表，值是用户对物品的偏好，一般是 [0,1] 或者 [-1, 1] 的浮点数值)。\n\u003e所谓减噪：用户行为数据是用户在使用应用过程中产生的，它可能存在大量的噪音和用户的误操作，我们可以通过经典的数据挖掘算法过滤掉行为数据中的噪音，这样可以是我们的分析更加精确（类似于网页的去噪处理）。\n\n\u003e所谓归一化：将各个行为的数据统一在一个相同的取值范围中，从而使得加权求和得到的总体喜好更加精确。最简单的归一化处理，便是将各类数据除以此类中的最大值，以保证归一化后的数据取值在 [0,1] 范围中。至于所谓的加权，很好理解，因为每个人占的权值不同，类似于一场唱歌比赛中对某几个选手进行投票决定其是否晋级，观众的投票抵1分，专家评委的投票抵5分，最后得分最多的选手直接晋级。\n\n3)找到相似用户或物品\n\n4）相似度计算公式方法，基于向量Vector的，其实也就是计算两个向量的距离，距离越近相似度越大。\n\n\u003e找物品间的相似度，用户不变，找多个用户对物品的评分；找用户间的相似度，物品不变，找用户对某些个物品的评分。\n\n5）而计算出来的这两个相似度则将作为基于用户、项目的两项协同过滤的推荐。\n\n- 欧几里得距离  $d(x,y)\u003d\\sqrt{\\sum(x_i-y_i)^2} $\n\n可以看出，当 n\u003d2 时，欧几里德距离就是平面上两个点的距离。当用欧几里德距离表示相似度，一般采用以下公式进行转换：距离越小，相似度越大（同时，避免除数为0）：\n\n$sim(x,y)\u003d\\frac{1}{1+d(x,y)}$\n\n- 余弦相似度 cosine-based similarity\n\n    - 两个项目 i ，j 视作为两个m维用户空间向量，相似度计算通过计算两个向量的余弦夹角，那么，对于m*n的评分矩阵，i ，j 的相似度sim( i , j ) 计算公式：$sim(i,j)\u003dcos(\\vec{i},\\vec{j})\u003d\\frac{\\vec{i}\\cdot \\vec{j}}{||\\vec{i}||_{2}\\ast ||\\vec{j}||_{2}}$\n（其中 \" · \"记做两个向量的内积）\n\n- 皮尔逊相关系数（person）\n\n    - 一般用于计算两个定距变量间联系的紧密程度，为了使计算结果精确，需要找出共同评分的用户。记用户集U为既评论了 i 又评论了 j 的用户集，那么对应的皮尔森相关系数计算公式为：\n    \n    $sim(i,j)\u003d\\frac{\\sum_{u \\in U}(R_{u,i}-\\bar R_i)(R_{u,i}-\\bar R_j)}{\\sqrt{\\sum_{u\\in U}(R_{u,i}-\\bar R_i)^2}\\sqrt{\\sum_{u\\in U}(R_{u,j}-\\bar R_j)^2}}$\n    其中 $R_{u,i}$为用户u对项目i的评分，对应的横杆为这个用户集U对项目i的评分。\n\n6）相似邻居计算。邻居分为两类：\n\n6.1、固定数量的邻居K-neighborhoods （或Fix-size neighborhoods），不论邻居的“远近”，只取最近的 K 个，作为其邻居，如下图A部分所示；\n\n6.2、基于相似度门槛的邻居，落在以当前点为中心，距离为 K 的区域中的所有点都作为当前点的邻居，如下图B部分所示。\n\n7）经过4)计算出来的基于用户的CF(基于用户推荐之用：通过共同口味与偏好找相似邻居用户，K-邻居算法，你朋友喜欢，你也可能喜欢)，基于物品的CF(基于项目推荐之用：发现物品之间的相似度，推荐类似的物品，你喜欢物品A，C与A相似，那么你可能也喜欢C)。\n\n![nerghborhoods](https://raw.githubusercontent.com/xmj-datawhale/interview/master/imgs/neighborhoods_catagory.png)\n\n2.2 基于基于用户相似度与项目相似度\n\n基于用户相似度与基于项目相似度计算的一个基本的区别是，基于用户相似度是基于评分矩阵中的行向量相似度求解，基于项目相似度计算式基于评分矩阵中列向量相似度求解，然后三个公式分别都可以适用，如下图：\n![nerghborhoods](https://raw.githubusercontent.com/xmj-datawhale/interview/master/imgs/recommend-cf-metrix.png)\n\n（其中，为0的表示未评分）\n基于项目相似度计算式计算如Item3，Item4两列向量相似度；\n基于用户相似度计算式计算如User3，User4量行向量相似度。\n\n如下图所示，所有的评分范围从1到5，5代表喜欢程度最高。第一个用户（行1）对第一本图书（列1）的评分是4，空的单元格表示用户未给图书评分。\n\n![user_item_rate_matrix](https://raw.githubusercontent.com/xmj-datawhale/interview/master/imgs/user_item_rate_matrix.png)\n\n对于大多数相似度量，向量之间相似度越高，代表彼此更相似。本例中，第一个用户第二、第三个用户非常相似，有两本共同书籍，与第四、第五个用户的相似度低一些，只有一本共同书籍，而与最后一名用户完全不相似，因为没有一本共同书籍。\n![user_item_rate_matrix](https://raw.githubusercontent.com/xmj-datawhale/interview/master/imgs/cosine_similarity_between.png)\n\n更一般的，我们可以计算出每个用户的相似性，并且在相似矩阵中表示它们。这是一个对称矩阵，单元格的背景颜色表明用户相似度的高低，更深的红色表示它们之间更相似。\n![cosine_similarity_between.png](https://raw.githubusercontent.com/xmj-datawhale/interview/master/imgs/cosine_similarity_between.png)\n\n所以，我们找到了与第一个用户最相似的第二个用户，删除用户已经评价过的书籍，给最相似用户正在阅读的书籍加权，然后计算出总和。\n\n在这种情况下，我们计算出n\u003d2，表示为了产生推荐，需要找出与目标用户最相似的两个用户，这两个用户分别是第二个和第三个用户，然后第一个用户已经评价了第一和第五本书，故产生的推荐书是第三本（4.5分），和第四本（3分）。\n![similarity_item_predition](https://raw.githubusercontent.com/xmj-datawhale/interview/master/imgs/similarity_item_predition.png)\n\n此外，什么时候用item-base，什么时候用user-base呢：[http://weibo.com/1580904460/zhZ9AiIkZ?mod\u003dweibotime](http://weibo.com/1580904460/zhZ9AiIkZ?mod\u003dweibotime)\n一般说来，如果item数目不多，比如不超过十万，而且不显著增长的话，就用item-based 好了。为何？如@wuzh670所说，如果item数目不多+不显著增长，说明item之间的关系在一段时间内相对稳定(对比user之间关系)，对于实时更新item-similarity需求就降低很多，推荐系统效率提高很多，故用item-based会明智些。\n反之，当item数目很多，建议用user-base。当然，实践中具体情况具体分析。如下图所示（摘自项亮的《推荐系统实践》一书）\n![UserCF\u0026ItemCF](https://raw.githubusercontent.com/xmj-datawhale/interview/master/imgs/UserCF\u0026ItemCF-explain.jpg)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "vec1-vec2 1.73205080757\nvec1-vec2 1.73205080757\nvec1-vec3 1.55563491861\n曼哈顿距离:vec1-vec2 3\n曼哈顿距离:vec1-vec3 2.4\n切比雪夫距离:vec1-vec2 1\n切比雪夫距离:vec1-vec3 1.0\n夹角余弦:vec1-vec2 0.997928889734\n夹角余弦:vec1-vec2 0.997928889734\n夹角余弦:vec1-vec3 0.973196794744\n0.707106781187\n0.707106781187\n1\n1.41421356237\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "#欧式距离\nimport numpy as np\nvec1 \u003d np.array([2,3,4])\nvec2 \u003d np.array([3,4,5])\nvec3 \u003d np.array([2.3,2.2,5.3])\ndis1\u003dnp.sqrt(np.sum(np.square(vec1-vec2)))\ndis11\u003dnp.linalg.norm(vec1-vec2)\nprint(\"vec1-vec2\",dis1)\nprint(\"vec1-vec2\",dis11)\ndis13\u003dnp.linalg.norm(vec1-vec3)\nprint(\"vec1-vec3\",dis13)\n#曼哈顿距离 d12\u003d|x1-x2|+|y1-y2|\nmht_dis\u003dnp.sum(np.abs(vec1-vec2))\nmht_dis2\u003dnp.linalg.norm(vec1-vec3,ord\u003d1)\nprint(\"曼哈顿距离:vec1-vec2\",mht_dis)\nprint(\"曼哈顿距离:vec1-vec3\",mht_dis2)\n#切比雪夫距离\nqbx_dis12\u003dnp.abs(vec1-vec2).max()\nqbx_dis13\u003dnp.linalg.norm(vec1-vec2,ord\u003dnp.inf)\nprint(\"切比雪夫距离:vec1-vec2\",qbx_dis12)\nprint(\"切比雪夫距离:vec1-vec3\",qbx_dis13)\n#夹角余弦\n\ncosine_dis12\u003dnp.dot(vec1,vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2))\ncosine_dis13\u003dnp.dot(vec1,vec3)/(np.linalg.norm(vec1)*np.linalg.norm(vec3))\ncosine_dis12_1\u003dnp.dot(vec1,vec2)/(np.sqrt(np.sum(np.square(vec1)))*np.sqrt(np.sum(np.square(vec2))))\nprint(\"夹角余弦:vec1-vec2\",cosine_dis12_1)\nprint(\"夹角余弦:vec1-vec2\",cosine_dis12)\nprint(\"夹角余弦:vec1-vec3\",cosine_dis13)\nvec1 \u003d np.array([0,1])\nvec2 \u003d np.array([1,1])\nprint(np.dot(vec1,vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2)))\nprint(1/np.sqrt(2))\nprint(np.dot(vec1,vec2))\nprint((np.sqrt(np.sum(np.square(vec1)))*np.sqrt(np.sum(np.square(vec2)))))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% \n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### jaccard similarity coefficient 杰卡德相似系数\n$J(A,B)\u003d\\frac{A\\cap B}{A\\cup B}$\n### jaccard dinstance 杰卡德距离\n$J_\\delta(A,B)\u003d1-J(A,B)\u003d\\frac{A\\cup B -A\\cap B }{A\\cup B}$\n\nP：样本A与B都是1的维度的个数\n\nq：样本A是1，样本B是0的维度的个数\n\nr：样本A是0，样本B是1的维度的个数\n\ns：样本A与B都是0的维度的个数\n\n那么样本A与B的杰卡德相似系数可以表示为：\n\n这里p+q+r可理解为A与B的并集的元素个数，而p是A与B的交集的元素个数。\n\n而样本A与B的杰卡德距离表示为：\n$\\frac{p}{p+q+r}$",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "[[1 1 0 0 1]\n [1 1 1 0 1]]\n杰卡德距离: [ 0.25]\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "import scipy.spatial.distance as dist\nvec1\u003dnp.array([1,1,0,0,1,])\nvec2\u003dnp.array([1,1,1,0,1,])\nmatv\u003dnp.array([vec1,vec2])\nprint(matv)\npdist \u003d dist.pdist(matv,\u0027jaccard\u0027)\nprint(\"杰卡德距离:\",pdist)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\nhttps://www.julyedu.com/question/big/kp_id/33/ques_id/2599\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}